{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Tensorflow 2.0: English - Russian\n",
    "## By: Jacob Gursky\n",
    "\n",
    "Hello all!  \n",
    "\n",
    "This project deals with a booming subfield of modern machine learning: **neural machine translation**, also called NMT.  NMT has seen huge gains over previous methods, and even has applications in other fields of machine learning, such as generating contextual embeddings.\n",
    "\n",
    "This tutorial assumes you already have some familiarity with neural networks, and more specifically Tensorflow and Keras.  \n",
    "\n",
    "So what exactly is NMT?  At a high level, neural machine translation takes advantage of sequence-to-sequence modeling, which attempts to generate a variable length sequence given an arbitrarily long input sequence.  This is a huge advantage over previous network designs, as we are no longer constrained by sequence length!  The network is typically designed with three portions, an encoder, a decoder, and an attention mechanism.  \n",
    "\n",
    "The encoder is usually constructed with an initial embedding layer and a recurrent layer or two in-between, producing a thought vector that is then passed to the decoder along with the last hidden state. The decoder is usually the mirror image: a recurrent layer or two with a corresponding dense layer that outputs the translated sequence.  The attention mechanism is a set of dense layers that produce a context vector that helps the network focus only on important words.  This usually helps with longer sequences.\n",
    "\n",
    "I won't get too in-depth on attention mechanisms, but here is a useful link if you are interested in learning more:\n",
    "\n",
    "http://akosiorek.github.io/ml/2017/10/14/visual-attention.html\n",
    "\n",
    "Okay, now that we have a brief understanding of NMT, we are going to dive into the code!\n",
    "\n",
    "If you want a more in-depth explanation of seq2seq models, the tensorflow notebook is fantastic!\n",
    "\n",
    "https://github.com/tensorflow/nmt\n",
    "\n",
    "Also, I would like to make clear that a good portion of the code used below comes from the offical Tensorflow 2.0 tutorial on NMT, though modified considerably. The link is provided below:\n",
    "\n",
    "https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb\n",
    "\n",
    "Furthermore, some code has been adapted from this article, which provides a much lower-level introduction to Seq2Seq modeling:\n",
    "\n",
    "https://towardsdatascience.com/seq2seq-model-in-tensorflow-ec0c557e560f\n",
    "\n",
    "### Loading Packages\n",
    "\n",
    "Below are the packages needed to run this notebook.  Note that we are using the Tensorflow 2.0 preview for this project, though you could also use 1.13 with eager execution enabled.  Note that as I am running this on my laptop, I do not have access to a GPU, though I would highly highly recommend using one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.0.0-dev20190305\n",
      "Using Eager Execution?:  True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "try:\n",
    "    tf.enable_eager_execution()\n",
    "except:\n",
    "    pass\n",
    "print('Tensorflow Version: ', tf.__version__)\n",
    "print('Using Eager Execution?: ', tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "# First lets make sure we are operating on GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "\n",
    "The data I am using for this tutorial comes from the last article I referenced above, which contains roughly 130K english and french sentence pairs that use a relatively small number of words, making it an excellent small dataset to play with!  However, we are only going to use the english sentences, and translate as many as we can into russian using the wonderful Yandex Translate API.\n",
    "\n",
    "## Defining our Helper Functions\n",
    "\n",
    "First we need to define our function to load our sentences from the needed text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our reading function for pulling in data\n",
    "def load_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a helper function to pad our punctutation with whitespaces so our model treats them as separate tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like we need to whitespace the punctuation\n",
    "def whitespace_punct(sent_list):\n",
    "    whitespaced = [re.sub('([.,!?;()\"])', r' \\1 ', x).strip() for x in sent_list]\n",
    "    whitespaced = [re.sub('\\s{2,}', ' ', x).strip() for x in whitespaced]\n",
    "    whitespaced = [x.replace('-',' - ') for x in whitespaced]\n",
    "    return whitespaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load our english sentences into memory and take a look at what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  9085267\n",
      "Number of sentences:  137861\n"
     ]
    }
   ],
   "source": [
    "# Importing our small english vocab\n",
    "eng = load_data('small_vocab_en.txt')\n",
    "print('Number of characters: ', len(eng))\n",
    "eng = eng.split('\\n')\n",
    "print('Number of sentences: ', len(eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Yandex Translate\n",
    "\n",
    "Now that we have our english sentences, we need to use the `yandex-translater` package to access the Yandex Translate API for Python to translate each of our sentences.  Note that there is a 10 million character free limit per month to use this API, so I will use as many sentences as can be translated in time!  Note that there are some requirements to using the translation results that can be seen here:\n",
    "\n",
    "https://tech.yandex.com/translate/doc/dg/concepts/design-requirements-docpage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Yandex translater to create our target corpus\n",
    "# Determining how many we need\n",
    "rus = load_data('small_vocab_ru.txt')\n",
    "rus = rus.split('\\n')\n",
    "rus_transl = rus\n",
    "\n",
    "start_word = len(rus_transl)\n",
    "print('Starting at position', start_word)\n",
    "\n",
    "# Doing the translation\n",
    "from yandex.Translater import Translater\n",
    "tr = Translater()\n",
    "tr.set_key('get your own API key!')\n",
    "tr.set_from_lang('en')\n",
    "tr.set_to_lang('ru')\n",
    "\n",
    "for i in range(start_word, len(eng)):\n",
    "    #print(i+1)\n",
    "    tr.set_text(eng[i])\n",
    "    rus_transl.append(tr.translate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have translated as many sentences as possible, we will save the results for persistence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our translations to a txt file\n",
    "if len(rus_transl) != 0:\n",
    "    f = open('small_vocab_ru.txt','w', encoding = 'utf-8')\n",
    "    f.write('\\n'.join(rus_transl))\n",
    "    f.close()\n",
    "    print('Saved Translated Data!')\n",
    "else:\n",
    "    print(\"Whoops, looks like the existing Russian corpus isn't loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load our data back into memory and take a look at a sentence pair to see if our data is aligned properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he thinks it's easy to translate english to portuguese .\n",
      "он думает, что это легко переводить с английского на португальский .\n"
     ]
    }
   ],
   "source": [
    "# Lets compare lines and make sure everything is correct\n",
    "eng_test = load_data('small_vocab_en.txt')\n",
    "rus_test = load_data('small_vocab_ru.txt')\n",
    "eng_test = eng_test.split('\\n')\n",
    "rus_test = rus_test.split('\\n')\n",
    "max_len = min([len(eng_test),len(rus_test)])\n",
    "random_line = random.sample(range(0,max_len),1)[0]\n",
    "print(eng_test[random_line])\n",
    "print(rus_test[random_line])\n",
    "# Everything looks fine and lined up, we probably just need more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powered by Yandex Translate\n",
    "\n",
    "http://translate.yandex.com/\n",
    "\n",
    "Looks like everything is aligned properly!  Now lets define some more helper functions to aid in setting up our models.  First we need a function to preprocess our sentences, removing whitespaces and adding the start and end tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(x):\n",
    "    # Making sure sentences are in lowercase and removing leading and trailing whitespaces\n",
    "    x = x.lower().rstrip().strip()\n",
    "    \n",
    "    # Adding our start and stop tokens\n",
    "    x = '<start> ' + x + ' <end>'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can bundle all of the above to create a helper function that neatly prepares all of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(eng_path, rus_path):\n",
    "    # First english\n",
    "    eng_corpus = load_data(eng_path)\n",
    "    eng_corpus = eng_corpus.split('\\n')\n",
    "    eng_corpus = [preprocess_sentence(x) for x in eng_corpus]\n",
    "    \n",
    "    # Now for Russian\n",
    "    rus_corpus = load_data(rus_path)\n",
    "    rus_corpus = rus_corpus.split('\\n')\n",
    "    rus_corpus = [preprocess_sentence(x) for x in rus_corpus]\n",
    "    \n",
    "    # Slimming down data as the lengths may differ\n",
    "    last_pair = min([len(eng_corpus),len(rus_corpus)])\n",
    "    eng_corpus, rus_corpus = eng_corpus[0:last_pair], rus_corpus[0:last_pair]\n",
    "\n",
    "    return eng_corpus, rus_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define a few more helper functions that will be used later, such as determing the max length of our sequences for padding, tokenizing our sentences, and another helper function that combines a few that we have defined already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    \n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "  \n",
    "    return tensor, lang_tokenizer\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(eng_path, rus_path):\n",
    "    # creating cleaned input, output pairs\n",
    "    inp_lang, targ_lang = create_dataset(eng_path, rus_path)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "Now we are finally ready to load our data in the format that we will feed into the NMT network!  We also use a train-test split of 20% for validation purposes using the scikit-learn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our data\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset('small_vocab_en.txt','small_vocab_ru.txt')\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6085, 6085, 1522, 1522)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our data neatly prepared, lets take a look at how the tokenization process is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "3 ----> <start>\n",
      "101 ----> we\n",
      "88 ----> dislike\n",
      "78 ----> lemons\n",
      "2 ----> ,\n",
      "84 ----> apples\n",
      "2 ----> ,\n",
      "11 ----> and\n",
      "81 ----> limes\n",
      "5 ----> .\n",
      "4 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "117 ----> мы\n",
      "7 ----> не\n",
      "155 ----> любим\n",
      "59 ----> лимоны\n",
      "4 ----> ,\n",
      "65 ----> яблоки\n",
      "8 ----> и\n",
      "85 ----> лаймы\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print (\"\\nTarget Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to declare some of our important model hyperparameters, such as number of epochs to train over, dropout rates, learning rate, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the buffer size as the number of training/validation obs\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "VAL_BUFFER_SIZE = len(input_tensor_val)\n",
    "\n",
    "# Setting our batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Number of epochs to train over\n",
    "EPOCHS = 50\n",
    "\n",
    "# Number of rounds with no improvement to stop after\n",
    "early_stopping_rounds = 5\n",
    "\n",
    "# How many steps do we need to take per epoch?\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "val_steps_per_epoch = len(input_tensor_val)//BATCH_SIZE\n",
    "\n",
    "# The dimension of our word embeddings\n",
    "embedding_dim = 256\n",
    "\n",
    "# The number of RNN cells to include in the recurrent layer\n",
    "units = 128\n",
    "\n",
    "# The dropout rate of the recurrent cells to help generalize\n",
    "dropout = 0.5\n",
    "\n",
    "# Determine the clipping threshold for our gradients to ease training\n",
    "gradient_clip = 1\n",
    "\n",
    "# Define the learning rate of our optimizer\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Setting vocab sizes\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Tensorflow's Dataset class to make our training process easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 19) (32, 20)\n",
      "(32, 19) (32, 20)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(VAL_BUFFER_SIZE)\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(example_input_batch.shape, example_target_batch.shape)\n",
    "example_input_batch, example_target_batch = next(iter(dataset_val))\n",
    "print(example_input_batch.shape, example_target_batch.shape)\n",
    "\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our NMT Model\n",
    "\n",
    "Now comes the fun stuff! We first need to define our encoder class, which takes in the input sequence and passes the thought vector and hidden state to the decoder.  We can do this easily using Keras's Model class.  Note that we are using a GRU layer hear instead of LSTM.  In my experience GRU performs nearly identically as long as you are using an attention mechanism, and is much faster to train!  We are only using a single layer here, but in larger scale projects you would use many more, sometimes even as a residual network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True, \n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.drop = tf.keras.layers.Dropout(rate=dropout)\n",
    "        \n",
    "    def call(self, x, hidden, dropout=False):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)     \n",
    "        if dropout:\n",
    "            output = self.drop(output)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our encoder object, using some of the hyperparameters defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define our Attention class!  I won't get too deep into attention mechanisms, but again, this helps our model with longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "  \n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    \n",
    "        return context_vector, attention_weights\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to declare the last part of our NMT model, which is the decoder.  Note again we are using a GRU layer instead of LSTM, and have dropout implemented to help make our model more generalizable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, \n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True, \n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.drop = tf.keras.layers.Dropout(rate=dropout)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output, dropout=False):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # Applying dropout\n",
    "        if dropout:\n",
    "            output = self.drop(output)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to declare our optimizer!  I opted for the ever-popular Adam optimizer, though there is much literature stating that SGD with momentum often compares comparably, even better sometimes!  I used Adam because I don't have the compute power with me right now to tune the momentum hyperparameter, so this works for our current needs.  We also define our loss function as sparse categorical cross-entropy, as opposed to the usual categorical cross-entropy, as our tokens are in integer format rather than one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate, clipvalue=gradient_clip)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its usually a good idea to save you model as you train, so we are going to set up a checkpoint directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Functions\n",
    "\n",
    "The last step we need to do before training is to define our training and validation tensorflow functions.  Note that for our training function, we use teacher forcing on the decoder, meaning that at each time-step it is fed the true value at the previous time-step to speed up training, while in the validation function we do not do this to get a better idea of model performance, as we cannot teacher force in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden, dropout=True)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)       \n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output, dropout=True)\n",
    "        \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "  \n",
    "    return batch_loss\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def calc_val_error(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "   \n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "        \n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']]*BATCH_SIZE, 1) \n",
    "        \n",
    "    for t in range(1, targ.shape[1]):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "        \n",
    "        loss += loss_function(targ[:, t], predictions)\n",
    "        \n",
    "        #dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        dec_input = tf.expand_dims(tf.argmax(predictions,1),1)\n",
    "\n",
    "    return loss/targ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together!\n",
    "Finally, the time has come to train our NMT model!  The process is relatively straight forward, but note that I also implemented early-stopping here to prevent our model from overfitting as we are using a relatively small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 | Train Loss = 2.5532 | Val Loss = 2.0912 | Train Time = 49.01 sec\n",
      "\n",
      "Epoch = 5 | Train Loss = 0.9926 | Val Loss = 2.2485 | Train Time = 12.75 sec\n",
      "\n",
      "Epoch = 10 | Train Loss = 0.6854 | Val Loss = 2.0896 | Train Time = 12.78 sec\n",
      "\n",
      "Epoch = 15 | Train Loss = 0.5625 | Val Loss = 1.8739 | Train Time = 12.87 sec\n",
      "\n",
      "Epoch = 20 | Train Loss = 0.4601 | Val Loss = 1.6985 | Train Time = 12.64 sec\n",
      "\n",
      "Epoch = 25 | Train Loss = 0.2926 | Val Loss = 1.4711 | Train Time = 12.62 sec\n",
      "\n",
      "Epoch = 30 | Train Loss = 0.2220 | Val Loss = 1.4201 | Train Time = 12.64 sec\n",
      "\n",
      "Epoch = 35 | Train Loss = 0.1628 | Val Loss = 1.3953 | Train Time = 12.83 sec\n",
      "\n",
      "Epoch = 40 | Train Loss = 0.1122 | Val Loss = 1.3606 | Train Time = 12.76 sec\n",
      "\n",
      "Epoch = 45 | Train Loss = 0.0851 | Val Loss = 1.3814 | Train Time = 12.86 sec\n",
      "\n",
      "Epoch = 50 | Train Loss = 0.0834 | Val Loss = 1.4094 | Train Time = 12.98 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounds_not_improved = 0\n",
    "prev_val_loss = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss, val_loss = 0, 0\n",
    "    \n",
    "    # Calculation loss and applying gradients on training batches\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "    # Calculating validation error for this epoch to determine early stopping\n",
    "    for (batch, (inp, targ)) in enumerate(dataset_val.take(val_steps_per_epoch)):\n",
    "        batch_loss = calc_val_error(inp, targ, enc_hidden)\n",
    "        val_loss += batch_loss\n",
    "        \n",
    "    # Creating our meaned losses\n",
    "    total_loss = total_loss/steps_per_epoch\n",
    "    val_loss = val_loss/val_steps_per_epoch\n",
    "    \n",
    "    if epoch+1 == 1 or (epoch+1)% 5==0:\n",
    "        print('Epoch = {} | Train Loss = {:.4f} | Val Loss = {:.4f} | Train Time = {:.2f} sec\\n'.format(epoch + 1,\n",
    "                                                                                                        total_loss,\n",
    "                                                                                                        val_loss,\n",
    "                                                                                                        time.time() - start))\n",
    "    \n",
    "    # We need to test for early stopping rounds\n",
    "    if val_loss>prev_val_loss:\n",
    "        rounds_not_improved += 1\n",
    "        if rounds_not_improved==5:\n",
    "            print('Epoch = {} | Train Loss = {:.4f} | Val Loss = {:.4f} | Train Time = {:.2f} sec\\n'.format(epoch + 1,\n",
    "                                                                                                            total_loss,\n",
    "                                                                                                            val_loss,\n",
    "                                                                                                            time.time() - start))\n",
    "            print('Early stopping limit reached!')\n",
    "            break\n",
    "    else:\n",
    "        rounds_not_improved = 0\n",
    "    prev_val_loss = val_loss\n",
    "    \n",
    "checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our Trained Model for Translation\n",
    "\n",
    "Now that we have our trained NMT model, we can use it to translate sentences from english to russian!  Note that we also need to define an evaluatation function that is very similar to our validation function defined above, as well as a translation wrapper to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], \n",
    "                                                           maxlen=max_length_inp, \n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    \n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence\n",
    "\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "        \n",
    "    print('Input: %s' % (sentence).encode('utf-8'))\n",
    "    print('Predicted translation: {}\\n'.format(result))\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model performance I took some english sentences that were not in either the training or validation set, so we can see how well our we are able to translate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: b'<start> california is usually beautiful during november , and it is never nice in april . <end>'\n",
      "Predicted translation: калифорния обычно прекрасный в ноябре , и это не приятно в апреле . <end> \n",
      "\n",
      "Input: b'<start> the peach is your least liked fruit , but the apple is our least liked . <end>'\n",
      "Predicted translation: персик ты не любил фрукты , но яблоко ты не любил фрукты , но яблоко ты не любил фрукты , \n",
      "\n",
      "Input: b'<start> he dislikes peaches , grapefruit , and lemons . <end>'\n",
      "Predicted translation: он не любит персики , грейпфруты и лимоны . <end> \n",
      "\n",
      "Input: b'<start> india is usually freezing during autumn , and it is usually cold in february . <end>'\n",
      "Predicted translation: индия обычно заморозки осенью , и это, как правило, холодно в феврале . <end> \n",
      "\n",
      "Input: b'<start> he dislikes limes , apples , and grapefruit . <end>'\n",
      "Predicted translation: он не любит лимоны , яблоки и грейпфруты . <end> \n",
      "\n",
      "Input: b'<start> we like apples and peaches . <end>'\n",
      "Predicted translation: мы как яблоки и яблоки . <end> \n",
      "\n",
      "Input: b'<start> this rabbit was her favorite animal . <end>'\n",
      "Predicted translation: этот лев был ее любимым животным . <end> \n",
      "\n",
      "Input: b'<start> france is sometimes snowy during july , and it is quiet in september . <end>'\n",
      "Predicted translation: франция снег в июле , и это тихо в сентябре . <end> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = ['california is usually beautiful during november , and it is never nice in april .',\n",
    "             'the peach is your least liked fruit , but the apple is our least liked .',\n",
    "             'he dislikes peaches , grapefruit , and lemons .',\n",
    "             'india is usually freezing during autumn , and it is usually cold in february .',\n",
    "             'he dislikes limes , apples , and grapefruit .',\n",
    "             'we like apples and peaches .',\n",
    "             'this rabbit was her favorite animal .',\n",
    "             'france is sometimes snowy during july , and it is quiet in september .']\n",
    "\n",
    "for i in sentences:\n",
    "    translate(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wow!** our model seems almost perfect even with such little data!  \n",
    "\n",
    "However, its probably time to discuss how this project could be improved:\n",
    "\n",
    "- Getting a larger and more diverse dataset\n",
    "- Using pretrained word embeddings\n",
    "- Using Stochastic Gradient Descent with Momentum\n",
    "\n",
    "If you have any questions/concerns about this project, feel free to send me an email at gursky021197@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
