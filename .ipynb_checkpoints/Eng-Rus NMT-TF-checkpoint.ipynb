{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# First lets make sure we are operating on GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our reading function for pulling in data\n",
    "def load_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like we need to whitespace the punctuation\n",
    "def whitespace_punct(sent_list):\n",
    "    whitespaced = [re.sub('([.,!?;()\"])', r' \\1 ', x).strip() for x in sent_list]\n",
    "    whitespaced = [re.sub('\\s{2,}', ' ', x).strip() for x in whitespaced]\n",
    "    whitespaced = [x.replace('-',' - ') for x in whitespaced]\n",
    "    return whitespaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  9085267\n",
      "Number of sentences:  137861\n"
     ]
    }
   ],
   "source": [
    "# Better yet, lets translate the small english corpus into russian using the Yandex Translate API\n",
    "# Importing our small english vocab\n",
    "eng = load_data('small_vocab_en.txt')\n",
    "print('Number of characters: ', len(eng))\n",
    "eng = eng.split('\\n')\n",
    "print('Number of sentences: ', len(eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at position:  11873\n",
      "11874\n",
      "11875\n",
      "11876\n",
      "11877\n",
      "11878\n",
      "11879\n",
      "11880\n",
      "11881\n",
      "11882\n",
      "11883\n",
      "11884\n",
      "11885\n",
      "11886\n",
      "11887\n",
      "11888\n",
      "11889\n",
      "11890\n",
      "11891\n",
      "11892\n",
      "11893\n",
      "11894\n",
      "11895\n",
      "11896\n",
      "11897\n",
      "11898\n",
      "11899\n",
      "11900\n",
      "11901\n",
      "11902\n",
      "11903\n",
      "11904\n",
      "11905\n",
      "11906\n",
      "11907\n",
      "11908\n",
      "11909\n",
      "11910\n",
      "11911\n",
      "11912\n",
      "11913\n",
      "11914\n",
      "11915\n",
      "11916\n",
      "11917\n",
      "11918\n",
      "11919\n",
      "11920\n",
      "11921\n",
      "11922\n",
      "11923\n",
      "11924\n",
      "11925\n",
      "11926\n",
      "11927\n",
      "11928\n",
      "11929\n",
      "11930\n",
      "11931\n",
      "11932\n",
      "11933\n",
      "11934\n",
      "11935\n",
      "11936\n",
      "11937\n",
      "11938\n",
      "11939\n",
      "11940\n",
      "11941\n",
      "11942\n",
      "11943\n",
      "11944\n",
      "11945\n",
      "11946\n",
      "11947\n",
      "11948\n",
      "11949\n",
      "11950\n",
      "11951\n",
      "11952\n",
      "11953\n",
      "11954\n",
      "11955\n",
      "11956\n",
      "11957\n",
      "11958\n",
      "11959\n",
      "11960\n",
      "11961\n",
      "11962\n",
      "11963\n",
      "11964\n",
      "11965\n",
      "11966\n",
      "11967\n",
      "11968\n",
      "11969\n",
      "11970\n",
      "11971\n",
      "11972\n",
      "11973\n",
      "11974\n",
      "11975\n",
      "11976\n",
      "11977\n",
      "11978\n",
      "11979\n",
      "11980\n",
      "11981\n",
      "11982\n",
      "11983\n",
      "11984\n",
      "11985\n",
      "11986\n",
      "11987\n",
      "11988\n",
      "11989\n",
      "11990\n",
      "11991\n",
      "11992\n",
      "11993\n",
      "11994\n",
      "11995\n",
      "11996\n",
      "11997\n",
      "11998\n",
      "11999\n",
      "12000\n",
      "12001\n",
      "12002\n",
      "12003\n",
      "12004\n",
      "12005\n",
      "12006\n",
      "12007\n",
      "12008\n",
      "12009\n",
      "12010\n",
      "12011\n",
      "12012\n",
      "12013\n",
      "12014\n",
      "12015\n",
      "12016\n",
      "12017\n",
      "12018\n",
      "12019\n",
      "12020\n",
      "12021\n",
      "12022\n",
      "12023\n",
      "12024\n",
      "12025\n",
      "12026\n",
      "12027\n",
      "12028\n",
      "12029\n",
      "12030\n",
      "12031\n",
      "12032\n",
      "12033\n",
      "12034\n",
      "12035\n",
      "12036\n",
      "12037\n",
      "12038\n",
      "12039\n",
      "12040\n",
      "12041\n",
      "12042\n",
      "12043\n",
      "12044\n",
      "12045\n",
      "12046\n",
      "12047\n",
      "12048\n",
      "12049\n",
      "12050\n",
      "12051\n",
      "12052\n",
      "12053\n",
      "12054\n",
      "12055\n",
      "12056\n",
      "12057\n",
      "12058\n",
      "12059\n",
      "12060\n",
      "12061\n",
      "12062\n",
      "12063\n",
      "12064\n",
      "12065\n",
      "12066\n",
      "12067\n",
      "12068\n",
      "12069\n",
      "12070\n",
      "12071\n",
      "12072\n",
      "12073\n",
      "12074\n",
      "12075\n",
      "12076\n",
      "12077\n",
      "12078\n",
      "12079\n",
      "12080\n",
      "12081\n",
      "12082\n",
      "12083\n",
      "12084\n",
      "12085\n",
      "12086\n",
      "12087\n",
      "12088\n",
      "12089\n",
      "12090\n",
      "12091\n",
      "12092\n",
      "12093\n",
      "12094\n",
      "12095\n",
      "12096\n",
      "12097\n",
      "12098\n",
      "12099\n",
      "12100\n",
      "12101\n",
      "12102\n",
      "12103\n",
      "12104\n",
      "12105\n",
      "12106\n",
      "12107\n",
      "12108\n",
      "12109\n",
      "12110\n",
      "12111\n",
      "12112\n",
      "12113\n",
      "12114\n",
      "12115\n",
      "12116\n",
      "12117\n",
      "12118\n",
      "12119\n",
      "12120\n",
      "12121\n",
      "12122\n",
      "12123\n",
      "12124\n",
      "12125\n",
      "12126\n",
      "12127\n",
      "12128\n",
      "12129\n",
      "12130\n",
      "12131\n",
      "12132\n",
      "12133\n",
      "12134\n",
      "12135\n",
      "12136\n",
      "12137\n",
      "12138\n",
      "12139\n",
      "12140\n",
      "12141\n",
      "12142\n",
      "12143\n",
      "12144\n",
      "12145\n",
      "12146\n",
      "12147\n",
      "12148\n",
      "12149\n",
      "12150\n",
      "12151\n",
      "12152\n",
      "12153\n",
      "12154\n",
      "12155\n",
      "12156\n",
      "12157\n",
      "12158\n",
      "12159\n",
      "12160\n",
      "12161\n",
      "12162\n",
      "12163\n",
      "12164\n",
      "12165\n",
      "12166\n",
      "12167\n",
      "12168\n",
      "12169\n",
      "12170\n",
      "12171\n",
      "12172\n",
      "12173\n",
      "12174\n",
      "12175\n",
      "12176\n",
      "12177\n",
      "12178\n",
      "12179\n",
      "12180\n",
      "12181\n",
      "12182\n",
      "12183\n",
      "12184\n",
      "12185\n",
      "12186\n",
      "12187\n",
      "12188\n",
      "12189\n",
      "12190\n",
      "12191\n",
      "12192\n",
      "12193\n",
      "12194\n",
      "12195\n",
      "12196\n",
      "12197\n",
      "12198\n",
      "12199\n",
      "12200\n",
      "12201\n",
      "12202\n",
      "12203\n",
      "12204\n",
      "12205\n",
      "12206\n",
      "12207\n",
      "12208\n",
      "12209\n",
      "12210\n",
      "12211\n",
      "12212\n",
      "12213\n",
      "12214\n",
      "12215\n",
      "12216\n",
      "12217\n",
      "12218\n",
      "12219\n",
      "12220\n",
      "12221\n",
      "12222\n",
      "12223\n",
      "12224\n",
      "12225\n",
      "12226\n",
      "12227\n",
      "12228\n",
      "12229\n",
      "12230\n",
      "12231\n",
      "12232\n",
      "12233\n",
      "12234\n",
      "12235\n",
      "12236\n",
      "12237\n",
      "12238\n",
      "12239\n",
      "12240\n",
      "12241\n",
      "12242\n",
      "12243\n",
      "12244\n",
      "12245\n",
      "12246\n",
      "12247\n",
      "12248\n",
      "12249\n",
      "12250\n",
      "12251\n",
      "12252\n",
      "12253\n",
      "12254\n",
      "12255\n",
      "12256\n",
      "12257\n",
      "12258\n",
      "12259\n",
      "12260\n",
      "12261\n",
      "12262\n",
      "12263\n",
      "12264\n",
      "12265\n",
      "12266\n",
      "12267\n",
      "12268\n",
      "12269\n",
      "12270\n",
      "12271\n",
      "12272\n",
      "12273\n",
      "12274\n",
      "12275\n",
      "12276\n",
      "12277\n",
      "12278\n",
      "12279\n",
      "12280\n",
      "12281\n",
      "12282\n",
      "12283\n",
      "12284\n",
      "12285\n",
      "12286\n",
      "12287\n",
      "12288\n",
      "12289\n",
      "12290\n",
      "12291\n",
      "12292\n",
      "12293\n",
      "12294\n",
      "12295\n",
      "12296\n",
      "12297\n",
      "12298\n",
      "12299\n",
      "12300\n",
      "12301\n",
      "12302\n",
      "12303\n",
      "12304\n",
      "12305\n",
      "12306\n",
      "12307\n",
      "12308\n",
      "12309\n",
      "12310\n",
      "12311\n",
      "12312\n",
      "12313\n",
      "12314\n",
      "12315\n",
      "12316\n",
      "12317\n",
      "12318\n",
      "12319\n",
      "12320\n",
      "12321\n",
      "12322\n",
      "12323\n",
      "12324\n",
      "12325\n",
      "12326\n",
      "12327\n",
      "12328\n",
      "12329\n",
      "12330\n",
      "12331\n",
      "12332\n",
      "12333\n",
      "12334\n",
      "12335\n",
      "12336\n",
      "12337\n",
      "12338\n",
      "12339\n",
      "12340\n",
      "12341\n",
      "12342\n",
      "12343\n",
      "12344\n",
      "12345\n",
      "12346\n",
      "12347\n",
      "12348\n",
      "12349\n",
      "12350\n",
      "12351\n",
      "12352\n",
      "12353\n",
      "12354\n",
      "12355\n",
      "12356\n",
      "12357\n",
      "12358\n",
      "12359\n",
      "12360\n",
      "12361\n",
      "12362\n",
      "12363\n",
      "12364\n",
      "12365\n",
      "12366\n",
      "12367\n",
      "12368\n",
      "12369\n",
      "12370\n",
      "12371\n",
      "12372\n",
      "12373\n",
      "12374\n",
      "12375\n",
      "12376\n",
      "12377\n",
      "12378\n",
      "12379\n",
      "12380\n",
      "12381\n",
      "12382\n",
      "12383\n",
      "12384\n",
      "12385\n",
      "12386\n",
      "12387\n",
      "12388\n",
      "12389\n",
      "12390\n",
      "12391\n",
      "12392\n",
      "12393\n",
      "12394\n",
      "12395\n",
      "12396\n",
      "12397\n",
      "12398\n",
      "12399\n",
      "12400\n",
      "12401\n",
      "12402\n",
      "12403\n",
      "12404\n",
      "12405\n",
      "12406\n",
      "12407\n",
      "12408\n",
      "12409\n",
      "12410\n",
      "12411\n",
      "12412\n",
      "12413\n",
      "12414\n",
      "12415\n",
      "12416\n",
      "12417\n",
      "12418\n",
      "12419\n",
      "12420\n",
      "12421\n",
      "12422\n",
      "12423\n",
      "12424\n",
      "12425\n",
      "12426\n",
      "12427\n",
      "12428\n",
      "12429\n",
      "12430\n",
      "12431\n",
      "12432\n",
      "12433\n",
      "12434\n",
      "12435\n",
      "12436\n",
      "12437\n",
      "12438\n",
      "12439\n",
      "12440\n",
      "12441\n",
      "12442\n",
      "12443\n",
      "12444\n",
      "12445\n",
      "12446\n",
      "12447\n",
      "12448\n",
      "12449\n",
      "12450\n",
      "12451\n",
      "12452\n",
      "12453\n",
      "12454\n",
      "12455\n",
      "12456\n",
      "12457\n",
      "12458\n",
      "12459\n",
      "12460\n",
      "12461\n",
      "12462\n",
      "12463\n",
      "12464\n",
      "12465\n",
      "12466\n",
      "12467\n",
      "12468\n",
      "12469\n",
      "12470\n",
      "12471\n",
      "12472\n",
      "12473\n",
      "12474\n",
      "12475\n",
      "12476\n",
      "12477\n",
      "12478\n",
      "12479\n",
      "12480\n",
      "12481\n",
      "12482\n",
      "12483\n",
      "12484\n",
      "12485\n",
      "12486\n",
      "12487\n",
      "12488\n",
      "12489\n",
      "12490\n",
      "12491\n",
      "12492\n",
      "12493\n",
      "12494\n",
      "12495\n",
      "12496\n",
      "12497\n",
      "12498\n",
      "12499\n",
      "12500\n",
      "12501\n",
      "12502\n",
      "12503\n",
      "12504\n",
      "12505\n",
      "12506\n",
      "12507\n",
      "12508\n",
      "12509\n",
      "12510\n",
      "12511\n",
      "12512\n",
      "12513\n",
      "12514\n",
      "12515\n",
      "12516\n",
      "12517\n",
      "12518\n",
      "12519\n",
      "12520\n",
      "12521\n",
      "12522\n",
      "12523\n",
      "12524\n",
      "12525\n",
      "12526\n",
      "12527\n",
      "12528\n",
      "12529\n",
      "12530\n",
      "12531\n",
      "12532\n",
      "12533\n",
      "12534\n",
      "12535\n",
      "12536\n",
      "12537\n",
      "12538\n",
      "12539\n",
      "12540\n",
      "12541\n",
      "12542\n",
      "12543\n",
      "12544\n",
      "12545\n",
      "12546\n",
      "12547\n",
      "12548\n",
      "12549\n",
      "12550\n",
      "12551\n",
      "12552\n",
      "12553\n",
      "12554\n",
      "12555\n",
      "12556\n",
      "12557\n",
      "12558\n",
      "12559\n",
      "12560\n",
      "12561\n",
      "12562\n",
      "12563\n",
      "12564\n",
      "12565\n",
      "12566\n",
      "12567\n",
      "12568\n",
      "12569\n",
      "12570\n",
      "12571\n",
      "12572\n",
      "12573\n",
      "12574\n",
      "12575\n",
      "12576\n",
      "12577\n",
      "12578\n",
      "12579\n",
      "12580\n",
      "12581\n",
      "12582\n",
      "12583\n",
      "12584\n",
      "12585\n",
      "12586\n",
      "12587\n",
      "12588\n",
      "12589\n",
      "12590\n",
      "12591\n",
      "12592\n",
      "12593\n",
      "12594\n",
      "12595\n",
      "12596\n",
      "12597\n",
      "12598\n",
      "12599\n",
      "12600\n",
      "12601\n",
      "12602\n",
      "12603\n",
      "12604\n",
      "12605\n",
      "12606\n",
      "12607\n",
      "12608\n",
      "12609\n",
      "12610\n",
      "12611\n",
      "12612\n",
      "12613\n",
      "12614\n",
      "12615\n",
      "12616\n",
      "12617\n",
      "12618\n",
      "12619\n",
      "12620\n",
      "12621\n",
      "12622\n",
      "12623\n",
      "12624\n",
      "12625\n",
      "12626\n",
      "12627\n",
      "12628\n",
      "12629\n",
      "12630\n",
      "12631\n",
      "12632\n",
      "12633\n",
      "12634\n",
      "12635\n",
      "12636\n",
      "12637\n",
      "12638\n",
      "12639\n",
      "12640\n",
      "12641\n",
      "12642\n",
      "12643\n",
      "12644\n",
      "12645\n",
      "12646\n",
      "12647\n",
      "12648\n",
      "12649\n",
      "12650\n",
      "12651\n",
      "12652\n",
      "12653\n",
      "12654\n",
      "12655\n",
      "12656\n",
      "12657\n",
      "12658\n",
      "12659\n",
      "12660\n",
      "12661\n",
      "12662\n",
      "12663\n",
      "12664\n",
      "12665\n",
      "12666\n",
      "12667\n",
      "12668\n",
      "12669\n",
      "12670\n",
      "12671\n",
      "12672\n",
      "12673\n",
      "12674\n",
      "12675\n",
      "12676\n",
      "12677\n",
      "12678\n",
      "12679\n",
      "12680\n",
      "12681\n",
      "12682\n",
      "12683\n",
      "12684\n",
      "12685\n",
      "12686\n",
      "12687\n",
      "12688\n",
      "12689\n",
      "12690\n",
      "12691\n",
      "12692\n",
      "12693\n",
      "12694\n",
      "12695\n",
      "12696\n",
      "12697\n",
      "12698\n",
      "12699\n",
      "12700\n",
      "12701\n",
      "12702\n",
      "12703\n",
      "12704\n",
      "12705\n",
      "12706\n",
      "12707\n",
      "12708\n",
      "12709\n",
      "12710\n",
      "12711\n",
      "12712\n",
      "12713\n",
      "12714\n",
      "12715\n",
      "12716\n",
      "12717\n",
      "12718\n",
      "12719\n",
      "12720\n",
      "12721\n",
      "12722\n",
      "12723\n",
      "12724\n",
      "12725\n",
      "12726\n",
      "12727\n",
      "12728\n",
      "12729\n",
      "12730\n",
      "12731\n",
      "12732\n",
      "12733\n",
      "12734\n",
      "12735\n",
      "12736\n",
      "12737\n",
      "12738\n",
      "12739\n",
      "12740\n",
      "12741\n",
      "12742\n",
      "12743\n",
      "12744\n",
      "12745\n",
      "12746\n",
      "12747\n",
      "12748\n",
      "12749\n",
      "12750\n",
      "12751\n",
      "12752\n",
      "12753\n",
      "12754\n",
      "12755\n",
      "12756\n",
      "12757\n",
      "12758\n",
      "12759\n",
      "12760\n",
      "12761\n",
      "12762\n",
      "12763\n",
      "12764\n",
      "12765\n",
      "12766\n",
      "12767\n",
      "12768\n",
      "12769\n",
      "12770\n",
      "12771\n",
      "12772\n",
      "12773\n",
      "12774\n",
      "12775\n",
      "12776\n",
      "12777\n",
      "12778\n",
      "12779\n",
      "12780\n",
      "12781\n",
      "12782\n",
      "12783\n",
      "12784\n",
      "12785\n",
      "12786\n",
      "12787\n",
      "12788\n",
      "12789\n",
      "12790\n",
      "12791\n",
      "12792\n",
      "12793\n",
      "12794\n",
      "12795\n",
      "12796\n",
      "12797\n",
      "12798\n",
      "12799\n",
      "12800\n",
      "12801\n",
      "12802\n",
      "12803\n",
      "12804\n",
      "12805\n",
      "12806\n",
      "12807\n",
      "12808\n",
      "12809\n",
      "12810\n",
      "12811\n",
      "12812\n",
      "12813\n",
      "12814\n",
      "12815\n",
      "12816\n",
      "12817\n",
      "12818\n",
      "12819\n",
      "12820\n",
      "12821\n",
      "12822\n",
      "12823\n",
      "12824\n",
      "12825\n",
      "12826\n",
      "12827\n",
      "12828\n",
      "12829\n",
      "12830\n",
      "12831\n",
      "12832\n",
      "12833\n",
      "12834\n",
      "12835\n",
      "12836\n",
      "12837\n",
      "12838\n",
      "12839\n",
      "12840\n",
      "12841\n",
      "12842\n",
      "12843\n",
      "12844\n",
      "12845\n",
      "12846\n",
      "12847\n",
      "12848\n",
      "12849\n",
      "12850\n",
      "12851\n",
      "12852\n",
      "12853\n",
      "12854\n",
      "12855\n",
      "12856\n",
      "12857\n",
      "12858\n",
      "12859\n",
      "12860\n",
      "12861\n",
      "12862\n",
      "12863\n",
      "12864\n",
      "12865\n",
      "12866\n",
      "12867\n",
      "12868\n",
      "12869\n",
      "12870\n",
      "12871\n",
      "12872\n",
      "12873\n",
      "12874\n",
      "12875\n",
      "12876\n",
      "12877\n",
      "12878\n",
      "12879\n",
      "12880\n",
      "12881\n",
      "12882\n",
      "12883\n",
      "12884\n",
      "12885\n",
      "12886\n",
      "12887\n",
      "12888\n",
      "12889\n",
      "12890\n",
      "12891\n",
      "12892\n",
      "12893\n",
      "12894\n",
      "12895\n",
      "12896\n",
      "12897\n",
      "12898\n",
      "12899\n",
      "12900\n",
      "12901\n",
      "12902\n",
      "12903\n",
      "12904\n",
      "12905\n",
      "12906\n",
      "12907\n",
      "12908\n",
      "12909\n",
      "12910\n",
      "12911\n",
      "12912\n",
      "12913\n",
      "12914\n",
      "12915\n",
      "12916\n",
      "12917\n",
      "12918\n",
      "12919\n",
      "12920\n",
      "12921\n",
      "12922\n",
      "12923\n",
      "12924\n",
      "12925\n",
      "12926\n",
      "12927\n",
      "12928\n",
      "12929\n",
      "12930\n",
      "12931\n",
      "12932\n",
      "12933\n",
      "12934\n",
      "12935\n",
      "12936\n",
      "12937\n",
      "12938\n",
      "12939\n",
      "12940\n",
      "12941\n",
      "12942\n",
      "12943\n",
      "12944\n",
      "12945\n",
      "12946\n",
      "12947\n",
      "12948\n",
      "12949\n",
      "12950\n",
      "12951\n",
      "12952\n",
      "12953\n",
      "12954\n",
      "12955\n",
      "12956\n",
      "12957\n",
      "12958\n",
      "12959\n",
      "12960\n",
      "12961\n",
      "12962\n",
      "12963\n",
      "12964\n",
      "12965\n",
      "12966\n",
      "12967\n",
      "12968\n",
      "12969\n",
      "12970\n",
      "12971\n",
      "12972\n",
      "12973\n",
      "12974\n",
      "12975\n",
      "12976\n",
      "12977\n",
      "12978\n",
      "12979\n",
      "12980\n",
      "12981\n",
      "12982\n",
      "12983\n",
      "12984\n",
      "12985\n",
      "12986\n",
      "12987\n",
      "12988\n",
      "12989\n",
      "12990\n",
      "12991\n",
      "12992\n",
      "12993\n",
      "12994\n",
      "12995\n",
      "12996\n",
      "12997\n",
      "12998\n",
      "12999\n",
      "13000\n",
      "13001\n",
      "13002\n",
      "13003\n",
      "13004\n",
      "13005\n",
      "13006\n",
      "13007\n",
      "13008\n",
      "13009\n",
      "13010\n",
      "13011\n",
      "13012\n",
      "13013\n",
      "13014\n",
      "13015\n",
      "13016\n",
      "13017\n",
      "13018\n",
      "13019\n",
      "13020\n",
      "13021\n",
      "13022\n",
      "13023\n",
      "13024\n",
      "13025\n",
      "13026\n",
      "13027\n",
      "13028\n",
      "13029\n",
      "13030\n",
      "13031\n",
      "13032\n",
      "13033\n",
      "13034\n",
      "13035\n",
      "13036\n",
      "13037\n",
      "13038\n",
      "13039\n",
      "13040\n",
      "13041\n",
      "13042\n",
      "13043\n",
      "13044\n",
      "13045\n",
      "13046\n",
      "13047\n",
      "13048\n",
      "13049\n",
      "13050\n",
      "13051\n",
      "13052\n",
      "13053\n",
      "13054\n",
      "13055\n",
      "13056\n",
      "13057\n",
      "13058\n",
      "13059\n",
      "13060\n",
      "13061\n",
      "13062\n",
      "13063\n",
      "13064\n",
      "13065\n",
      "13066\n",
      "13067\n",
      "13068\n",
      "13069\n",
      "13070\n",
      "13071\n",
      "13072\n",
      "13073\n",
      "13074\n",
      "13075\n",
      "13076\n",
      "13077\n",
      "13078\n",
      "13079\n",
      "13080\n",
      "13081\n",
      "13082\n",
      "13083\n",
      "13084\n",
      "13085\n",
      "13086\n",
      "13087\n",
      "13088\n",
      "13089\n",
      "13090\n",
      "13091\n",
      "13092\n",
      "13093\n",
      "13094\n",
      "13095\n",
      "13096\n",
      "13097\n",
      "13098\n",
      "13099\n",
      "13100\n",
      "13101\n",
      "13102\n",
      "13103\n",
      "13104\n",
      "13105\n",
      "13106\n",
      "13107\n",
      "13108\n",
      "13109\n",
      "13110\n",
      "13111\n",
      "13112\n",
      "13113\n",
      "13114\n",
      "13115\n",
      "13116\n",
      "13117\n",
      "13118\n",
      "13119\n",
      "13120\n",
      "13121\n",
      "13122\n",
      "13123\n",
      "13124\n",
      "13125\n",
      "13126\n",
      "13127\n",
      "13128\n",
      "13129\n",
      "13130\n",
      "13131\n",
      "13132\n",
      "13133\n",
      "13134\n",
      "13135\n",
      "13136\n",
      "13137\n",
      "13138\n",
      "13139\n",
      "13140\n",
      "13141\n",
      "13142\n",
      "13143\n",
      "13144\n",
      "13145\n",
      "13146\n",
      "13147\n",
      "13148\n",
      "13149\n",
      "13150\n",
      "13151\n",
      "13152\n",
      "13153\n",
      "13154\n",
      "13155\n",
      "13156\n",
      "13157\n",
      "13158\n",
      "13159\n",
      "13160\n",
      "13161\n",
      "13162\n",
      "13163\n",
      "13164\n",
      "13165\n",
      "13166\n",
      "13167\n",
      "13168\n",
      "13169\n",
      "13170\n",
      "13171\n",
      "13172\n",
      "13173\n",
      "13174\n",
      "13175\n",
      "13176\n",
      "13177\n",
      "13178\n",
      "13179\n",
      "13180\n",
      "13181\n",
      "13182\n",
      "13183\n",
      "13184\n",
      "13185\n",
      "13186\n",
      "13187\n",
      "13188\n",
      "13189\n",
      "13190\n",
      "13191\n",
      "13192\n",
      "13193\n",
      "13194\n",
      "13195\n",
      "13196\n",
      "13197\n",
      "13198\n",
      "13199\n",
      "13200\n",
      "13201\n",
      "13202\n",
      "13203\n",
      "13204\n",
      "13205\n",
      "13206\n",
      "13207\n",
      "13208\n",
      "13209\n",
      "13210\n",
      "13211\n",
      "13212\n",
      "13213\n",
      "13214\n",
      "13215\n",
      "13216\n",
      "13217\n",
      "13218\n",
      "13219\n",
      "13220\n",
      "13221\n",
      "13222\n",
      "13223\n",
      "13224\n",
      "13225\n",
      "13226\n",
      "13227\n",
      "13228\n",
      "13229\n",
      "13230\n",
      "13231\n",
      "13232\n",
      "13233\n",
      "13234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13235\n",
      "13236\n",
      "13237\n",
      "13238\n",
      "13239\n",
      "13240\n",
      "13241\n",
      "13242\n",
      "13243\n",
      "13244\n",
      "13245\n",
      "13246\n",
      "13247\n",
      "13248\n",
      "13249\n",
      "13250\n",
      "13251\n",
      "13252\n",
      "13253\n",
      "13254\n",
      "13255\n",
      "13256\n",
      "13257\n",
      "13258\n",
      "13259\n",
      "13260\n",
      "13261\n",
      "13262\n",
      "13263\n",
      "13264\n",
      "13265\n",
      "13266\n",
      "13267\n",
      "13268\n",
      "13269\n",
      "13270\n",
      "13271\n",
      "13272\n",
      "13273\n",
      "13274\n",
      "13275\n",
      "13276\n",
      "13277\n",
      "13278\n",
      "13279\n",
      "13280\n",
      "13281\n",
      "13282\n",
      "13283\n",
      "13284\n",
      "13285\n",
      "13286\n",
      "13287\n",
      "13288\n",
      "13289\n",
      "13290\n",
      "13291\n",
      "13292\n",
      "13293\n",
      "13294\n",
      "13295\n",
      "13296\n",
      "13297\n",
      "13298\n",
      "13299\n",
      "13300\n",
      "13301\n",
      "13302\n",
      "13303\n",
      "13304\n",
      "13305\n",
      "13306\n",
      "13307\n",
      "13308\n",
      "13309\n",
      "13310\n",
      "13311\n",
      "13312\n",
      "13313\n",
      "13314\n",
      "13315\n",
      "13316\n",
      "13317\n",
      "13318\n",
      "13319\n",
      "13320\n",
      "13321\n",
      "13322\n",
      "13323\n",
      "13324\n",
      "13325\n",
      "13326\n",
      "13327\n",
      "13328\n",
      "13329\n",
      "13330\n",
      "13331\n",
      "13332\n",
      "13333\n",
      "13334\n",
      "13335\n",
      "13336\n",
      "13337\n",
      "13338\n",
      "13339\n",
      "13340\n",
      "13341\n",
      "13342\n",
      "13343\n",
      "13344\n",
      "13345\n",
      "13346\n",
      "13347\n",
      "13348\n",
      "13349\n",
      "13350\n",
      "13351\n",
      "13352\n",
      "13353\n",
      "13354\n",
      "13355\n",
      "13356\n",
      "13357\n",
      "13358\n",
      "13359\n",
      "13360\n",
      "13361\n",
      "13362\n",
      "13363\n",
      "13364\n",
      "13365\n",
      "13366\n",
      "13367\n",
      "13368\n",
      "13369\n",
      "13370\n",
      "13371\n",
      "13372\n",
      "13373\n",
      "13374\n",
      "13375\n",
      "13376\n",
      "13377\n",
      "13378\n",
      "13379\n",
      "13380\n",
      "13381\n",
      "13382\n",
      "13383\n",
      "13384\n",
      "13385\n",
      "13386\n",
      "13387\n",
      "13388\n",
      "13389\n",
      "13390\n",
      "13391\n",
      "13392\n",
      "13393\n",
      "13394\n",
      "13395\n",
      "13396\n",
      "13397\n",
      "13398\n",
      "13399\n",
      "13400\n",
      "13401\n",
      "13402\n",
      "13403\n",
      "13404\n",
      "13405\n",
      "13406\n",
      "13407\n",
      "13408\n",
      "13409\n",
      "13410\n",
      "13411\n",
      "13412\n",
      "13413\n",
      "13414\n",
      "13415\n",
      "13416\n",
      "13417\n",
      "13418\n",
      "13419\n",
      "13420\n",
      "13421\n",
      "13422\n",
      "13423\n",
      "13424\n",
      "13425\n",
      "13426\n",
      "13427\n",
      "13428\n",
      "13429\n",
      "13430\n",
      "13431\n",
      "13432\n",
      "13433\n",
      "13434\n",
      "13435\n",
      "13436\n",
      "13437\n",
      "13438\n",
      "13439\n",
      "13440\n",
      "13441\n",
      "13442\n",
      "13443\n",
      "13444\n",
      "13445\n",
      "13446\n",
      "13447\n",
      "13448\n",
      "13449\n",
      "13450\n",
      "13451\n",
      "13452\n",
      "13453\n",
      "13454\n",
      "13455\n",
      "13456\n",
      "13457\n",
      "13458\n",
      "13459\n",
      "13460\n",
      "13461\n",
      "13462\n",
      "13463\n",
      "13464\n",
      "13465\n",
      "13466\n",
      "13467\n",
      "13468\n",
      "13469\n",
      "13470\n",
      "13471\n",
      "13472\n",
      "13473\n",
      "13474\n",
      "13475\n",
      "13476\n",
      "13477\n",
      "13478\n",
      "13479\n",
      "13480\n",
      "13481\n",
      "13482\n",
      "13483\n",
      "13484\n",
      "13485\n",
      "13486\n",
      "13487\n",
      "13488\n",
      "13489\n",
      "13490\n",
      "13491\n",
      "13492\n",
      "13493\n",
      "13494\n",
      "13495\n",
      "13496\n",
      "13497\n",
      "13498\n",
      "13499\n",
      "13500\n",
      "13501\n",
      "13502\n",
      "13503\n",
      "13504\n",
      "13505\n",
      "13506\n",
      "13507\n",
      "13508\n",
      "13509\n",
      "13510\n",
      "13511\n",
      "13512\n",
      "13513\n",
      "13514\n",
      "13515\n",
      "13516\n",
      "13517\n",
      "13518\n",
      "13519\n",
      "13520\n",
      "13521\n",
      "13522\n",
      "13523\n",
      "13524\n",
      "13525\n",
      "13526\n",
      "13527\n",
      "13528\n",
      "13529\n",
      "13530\n",
      "13531\n",
      "13532\n",
      "13533\n",
      "13534\n",
      "13535\n",
      "13536\n",
      "13537\n",
      "13538\n",
      "13539\n",
      "13540\n",
      "13541\n",
      "13542\n",
      "13543\n",
      "13544\n",
      "13545\n",
      "13546\n",
      "13547\n",
      "13548\n",
      "13549\n",
      "13550\n",
      "13551\n",
      "13552\n",
      "13553\n",
      "13554\n",
      "13555\n",
      "13556\n",
      "13557\n",
      "13558\n",
      "13559\n",
      "13560\n",
      "13561\n",
      "13562\n",
      "13563\n",
      "13564\n",
      "13565\n",
      "13566\n",
      "13567\n",
      "13568\n",
      "13569\n",
      "13570\n",
      "13571\n",
      "13572\n",
      "13573\n",
      "13574\n",
      "13575\n",
      "13576\n",
      "13577\n",
      "13578\n",
      "13579\n",
      "13580\n",
      "13581\n",
      "13582\n",
      "13583\n",
      "13584\n",
      "13585\n",
      "13586\n",
      "13587\n",
      "13588\n",
      "13589\n",
      "13590\n",
      "13591\n",
      "13592\n",
      "13593\n",
      "13594\n",
      "13595\n",
      "13596\n",
      "13597\n",
      "13598\n",
      "13599\n",
      "13600\n",
      "13601\n",
      "13602\n",
      "13603\n",
      "13604\n",
      "13605\n",
      "13606\n",
      "13607\n",
      "13608\n",
      "13609\n",
      "13610\n",
      "13611\n",
      "13612\n",
      "13613\n",
      "13614\n",
      "13615\n",
      "13616\n",
      "13617\n",
      "13618\n",
      "13619\n",
      "13620\n",
      "13621\n",
      "13622\n",
      "13623\n",
      "13624\n",
      "13625\n",
      "13626\n",
      "13627\n",
      "13628\n",
      "13629\n",
      "13630\n",
      "13631\n",
      "13632\n",
      "13633\n",
      "13634\n",
      "13635\n",
      "13636\n",
      "13637\n",
      "13638\n",
      "13639\n",
      "13640\n",
      "13641\n",
      "13642\n",
      "13643\n",
      "13644\n",
      "13645\n",
      "13646\n",
      "13647\n",
      "13648\n",
      "13649\n",
      "13650\n",
      "13651\n",
      "13652\n",
      "13653\n",
      "13654\n",
      "13655\n",
      "13656\n",
      "13657\n",
      "13658\n",
      "13659\n",
      "13660\n",
      "13661\n",
      "13662\n",
      "13663\n",
      "13664\n",
      "13665\n",
      "13666\n",
      "13667\n",
      "13668\n",
      "13669\n",
      "13670\n",
      "13671\n",
      "13672\n",
      "13673\n",
      "13674\n",
      "13675\n",
      "13676\n",
      "13677\n",
      "13678\n",
      "13679\n",
      "13680\n",
      "13681\n",
      "13682\n",
      "13683\n",
      "13684\n",
      "13685\n",
      "13686\n",
      "13687\n",
      "13688\n",
      "13689\n",
      "13690\n",
      "13691\n",
      "13692\n",
      "13693\n",
      "13694\n",
      "13695\n",
      "13696\n",
      "13697\n",
      "13698\n",
      "13699\n",
      "13700\n",
      "13701\n",
      "13702\n",
      "13703\n",
      "13704\n",
      "13705\n",
      "13706\n",
      "13707\n",
      "13708\n",
      "13709\n",
      "13710\n",
      "13711\n",
      "13712\n",
      "13713\n",
      "13714\n",
      "13715\n",
      "13716\n",
      "13717\n",
      "13718\n",
      "13719\n",
      "13720\n",
      "13721\n",
      "13722\n",
      "13723\n",
      "13724\n",
      "13725\n",
      "13726\n",
      "13727\n",
      "13728\n",
      "13729\n",
      "13730\n",
      "13731\n",
      "13732\n",
      "13733\n",
      "13734\n",
      "13735\n",
      "13736\n",
      "13737\n",
      "13738\n",
      "13739\n",
      "13740\n",
      "13741\n",
      "13742\n",
      "13743\n",
      "13744\n",
      "13745\n",
      "13746\n",
      "13747\n",
      "13748\n",
      "13749\n",
      "13750\n",
      "13751\n",
      "13752\n",
      "13753\n",
      "13754\n",
      "13755\n",
      "13756\n",
      "13757\n",
      "13758\n",
      "13759\n",
      "13760\n",
      "13761\n",
      "13762\n",
      "13763\n",
      "13764\n",
      "13765\n",
      "13766\n",
      "13767\n",
      "13768\n",
      "13769\n",
      "13770\n",
      "13771\n",
      "13772\n",
      "13773\n",
      "13774\n",
      "13775\n",
      "13776\n",
      "13777\n",
      "13778\n",
      "13779\n",
      "13780\n",
      "13781\n",
      "13782\n",
      "13783\n",
      "13784\n",
      "13785\n",
      "13786\n",
      "13787\n",
      "13788\n",
      "13789\n",
      "13790\n",
      "13791\n",
      "13792\n",
      "13793\n",
      "13794\n",
      "13795\n",
      "13796\n",
      "13797\n",
      "13798\n",
      "13799\n",
      "13800\n",
      "13801\n",
      "13802\n",
      "13803\n",
      "13804\n",
      "13805\n",
      "13806\n",
      "13807\n",
      "13808\n",
      "13809\n",
      "13810\n",
      "13811\n",
      "13812\n",
      "13813\n",
      "13814\n",
      "13815\n",
      "13816\n",
      "13817\n",
      "13818\n",
      "13819\n",
      "13820\n",
      "13821\n",
      "13822\n",
      "13823\n",
      "13824\n",
      "13825\n",
      "13826\n",
      "13827\n",
      "13828\n",
      "13829\n",
      "13830\n",
      "13831\n",
      "13832\n",
      "13833\n",
      "13834\n",
      "13835\n",
      "13836\n",
      "13837\n",
      "13838\n",
      "13839\n",
      "13840\n",
      "13841\n",
      "13842\n",
      "13843\n",
      "13844\n",
      "13845\n",
      "13846\n",
      "13847\n",
      "13848\n",
      "13849\n",
      "13850\n",
      "13851\n",
      "13852\n",
      "13853\n",
      "13854\n",
      "13855\n",
      "13856\n",
      "13857\n",
      "13858\n",
      "13859\n",
      "13860\n",
      "13861\n",
      "13862\n",
      "13863\n",
      "13864\n",
      "13865\n",
      "13866\n",
      "13867\n",
      "13868\n",
      "13869\n",
      "13870\n",
      "13871\n",
      "13872\n",
      "13873\n",
      "13874\n",
      "13875\n",
      "13876\n",
      "13877\n",
      "13878\n",
      "13879\n",
      "13880\n",
      "13881\n",
      "13882\n",
      "13883\n",
      "13884\n",
      "13885\n",
      "13886\n",
      "13887\n",
      "13888\n",
      "13889\n",
      "13890\n",
      "13891\n",
      "13892\n",
      "13893\n",
      "13894\n",
      "13895\n",
      "13896\n",
      "13897\n",
      "13898\n",
      "13899\n",
      "13900\n",
      "13901\n",
      "13902\n",
      "13903\n",
      "13904\n",
      "13905\n",
      "13906\n",
      "13907\n",
      "13908\n",
      "13909\n",
      "13910\n",
      "13911\n",
      "13912\n",
      "13913\n",
      "13914\n",
      "13915\n",
      "13916\n",
      "13917\n",
      "13918\n",
      "13919\n",
      "13920\n",
      "13921\n",
      "13922\n",
      "13923\n",
      "13924\n",
      "13925\n",
      "13926\n",
      "13927\n",
      "13928\n",
      "13929\n",
      "13930\n",
      "13931\n",
      "13932\n",
      "13933\n",
      "13934\n",
      "13935\n",
      "13936\n",
      "13937\n",
      "13938\n",
      "13939\n",
      "13940\n",
      "13941\n",
      "13942\n",
      "13943\n",
      "13944\n",
      "13945\n",
      "13946\n",
      "13947\n",
      "13948\n",
      "13949\n",
      "13950\n",
      "13951\n",
      "13952\n",
      "13953\n",
      "13954\n",
      "13955\n",
      "13956\n",
      "13957\n",
      "13958\n",
      "13959\n",
      "13960\n",
      "13961\n",
      "13962\n",
      "13963\n",
      "13964\n",
      "13965\n",
      "13966\n",
      "13967\n",
      "13968\n",
      "13969\n",
      "13970\n",
      "13971\n",
      "13972\n",
      "13973\n",
      "13974\n",
      "13975\n",
      "13976\n",
      "13977\n",
      "13978\n",
      "13979\n",
      "13980\n",
      "13981\n",
      "13982\n",
      "13983\n",
      "13984\n",
      "13985\n",
      "13986\n",
      "13987\n",
      "13988\n",
      "13989\n",
      "13990\n",
      "13991\n",
      "13992\n",
      "13993\n",
      "13994\n",
      "13995\n",
      "13996\n",
      "13997\n",
      "13998\n",
      "13999\n",
      "14000\n",
      "14001\n",
      "14002\n",
      "14003\n",
      "14004\n",
      "14005\n",
      "14006\n",
      "14007\n",
      "14008\n",
      "14009\n",
      "14010\n",
      "14011\n",
      "14012\n",
      "14013\n",
      "14014\n",
      "14015\n",
      "14016\n",
      "14017\n",
      "14018\n",
      "14019\n",
      "14020\n",
      "14021\n",
      "14022\n",
      "14023\n",
      "14024\n",
      "14025\n",
      "14026\n",
      "14027\n",
      "14028\n",
      "14029\n",
      "14030\n",
      "14031\n",
      "14032\n",
      "14033\n",
      "14034\n",
      "14035\n",
      "14036\n",
      "14037\n",
      "14038\n",
      "14039\n",
      "14040\n",
      "14041\n",
      "14042\n",
      "14043\n",
      "14044\n",
      "14045\n",
      "14046\n",
      "14047\n",
      "14048\n",
      "14049\n",
      "14050\n",
      "14051\n",
      "14052\n",
      "14053\n",
      "14054\n",
      "14055\n",
      "14056\n",
      "14057\n",
      "14058\n",
      "14059\n",
      "14060\n",
      "14061\n",
      "14062\n",
      "14063\n",
      "14064\n",
      "14065\n",
      "14066\n",
      "14067\n",
      "14068\n",
      "14069\n",
      "14070\n",
      "14071\n",
      "14072\n",
      "14073\n",
      "14074\n",
      "14075\n",
      "14076\n",
      "14077\n",
      "14078\n",
      "14079\n",
      "14080\n",
      "14081\n",
      "14082\n",
      "14083\n",
      "14084\n",
      "14085\n",
      "14086\n",
      "14087\n",
      "14088\n",
      "14089\n",
      "14090\n",
      "14091\n",
      "14092\n",
      "14093\n",
      "14094\n",
      "14095\n",
      "14096\n",
      "14097\n",
      "14098\n",
      "14099\n",
      "14100\n",
      "14101\n",
      "14102\n",
      "14103\n",
      "14104\n",
      "14105\n",
      "14106\n",
      "14107\n",
      "14108\n",
      "14109\n",
      "14110\n",
      "14111\n",
      "14112\n",
      "14113\n",
      "14114\n",
      "14115\n",
      "14116\n",
      "14117\n",
      "14118\n",
      "14119\n",
      "14120\n",
      "14121\n",
      "14122\n",
      "14123\n",
      "14124\n",
      "14125\n",
      "14126\n",
      "14127\n",
      "14128\n",
      "14129\n",
      "14130\n",
      "14131\n",
      "14132\n",
      "14133\n",
      "14134\n",
      "14135\n",
      "14136\n",
      "14137\n",
      "14138\n",
      "14139\n",
      "14140\n",
      "14141\n",
      "14142\n",
      "14143\n",
      "14144\n",
      "14145\n",
      "14146\n",
      "14147\n",
      "14148\n",
      "14149\n",
      "14150\n",
      "14151\n",
      "14152\n",
      "14153\n",
      "14154\n",
      "14155\n",
      "14156\n",
      "14157\n",
      "14158\n",
      "14159\n",
      "14160\n",
      "14161\n",
      "14162\n",
      "14163\n",
      "14164\n",
      "14165\n",
      "14166\n",
      "14167\n",
      "14168\n",
      "14169\n",
      "14170\n",
      "14171\n",
      "14172\n",
      "14173\n",
      "14174\n",
      "14175\n",
      "14176\n",
      "14177\n",
      "14178\n",
      "14179\n",
      "14180\n",
      "14181\n",
      "14182\n",
      "14183\n",
      "14184\n",
      "14185\n",
      "14186\n",
      "14187\n",
      "14188\n",
      "14189\n",
      "14190\n",
      "14191\n",
      "14192\n",
      "14193\n",
      "14194\n",
      "14195\n",
      "14196\n",
      "14197\n",
      "14198\n",
      "14199\n",
      "14200\n",
      "14201\n",
      "14202\n",
      "14203\n",
      "14204\n",
      "14205\n",
      "14206\n",
      "14207\n",
      "14208\n",
      "14209\n",
      "14210\n",
      "14211\n",
      "14212\n",
      "14213\n",
      "14214\n",
      "14215\n",
      "14216\n",
      "14217\n",
      "14218\n",
      "14219\n",
      "14220\n",
      "14221\n",
      "14222\n",
      "14223\n",
      "14224\n",
      "14225\n",
      "14226\n",
      "14227\n",
      "14228\n",
      "14229\n",
      "14230\n",
      "14231\n",
      "14232\n",
      "14233\n",
      "14234\n",
      "14235\n",
      "14236\n",
      "14237\n",
      "14238\n",
      "14239\n",
      "14240\n",
      "14241\n",
      "14242\n",
      "14243\n",
      "14244\n",
      "14245\n",
      "14246\n",
      "14247\n",
      "14248\n",
      "14249\n",
      "14250\n",
      "14251\n",
      "14252\n",
      "14253\n",
      "14254\n",
      "14255\n",
      "14256\n",
      "14257\n",
      "14258\n",
      "14259\n",
      "14260\n",
      "14261\n",
      "14262\n",
      "14263\n",
      "14264\n",
      "14265\n",
      "14266\n",
      "14267\n",
      "14268\n",
      "14269\n",
      "14270\n",
      "14271\n",
      "14272\n",
      "14273\n",
      "14274\n",
      "14275\n",
      "14276\n",
      "14277\n",
      "14278\n",
      "14279\n",
      "14280\n",
      "14281\n",
      "14282\n",
      "14283\n",
      "14284\n",
      "14285\n",
      "14286\n",
      "14287\n",
      "14288\n",
      "14289\n",
      "14290\n",
      "14291\n",
      "14292\n",
      "14293\n",
      "14294\n",
      "14295\n",
      "14296\n",
      "14297\n",
      "14298\n",
      "14299\n",
      "14300\n",
      "14301\n",
      "14302\n",
      "14303\n",
      "14304\n",
      "14305\n",
      "14306\n",
      "14307\n",
      "14308\n",
      "14309\n",
      "14310\n",
      "14311\n",
      "14312\n",
      "14313\n",
      "14314\n",
      "14315\n",
      "14316\n",
      "14317\n",
      "14318\n",
      "14319\n",
      "14320\n",
      "14321\n",
      "14322\n",
      "14323\n",
      "14324\n",
      "14325\n",
      "14326\n",
      "14327\n",
      "14328\n",
      "14329\n",
      "14330\n",
      "14331\n",
      "14332\n",
      "14333\n",
      "14334\n",
      "14335\n",
      "14336\n",
      "14337\n",
      "14338\n",
      "14339\n",
      "14340\n",
      "14341\n",
      "14342\n",
      "14343\n",
      "14344\n",
      "14345\n",
      "14346\n",
      "14347\n",
      "14348\n",
      "14349\n",
      "14350\n",
      "14351\n",
      "14352\n",
      "14353\n",
      "14354\n",
      "14355\n",
      "14356\n",
      "14357\n",
      "14358\n",
      "14359\n",
      "14360\n",
      "14361\n",
      "14362\n",
      "14363\n",
      "14364\n",
      "14365\n",
      "14366\n",
      "14367\n",
      "14368\n",
      "14369\n",
      "14370\n",
      "14371\n",
      "14372\n",
      "14373\n",
      "14374\n",
      "14375\n",
      "14376\n",
      "14377\n",
      "14378\n",
      "14379\n",
      "14380\n",
      "14381\n",
      "14382\n",
      "14383\n",
      "14384\n",
      "14385\n",
      "14386\n",
      "14387\n",
      "14388\n",
      "14389\n",
      "14390\n",
      "14391\n",
      "14392\n",
      "14393\n",
      "14394\n",
      "14395\n",
      "14396\n",
      "14397\n",
      "14398\n",
      "14399\n",
      "14400\n",
      "14401\n",
      "14402\n",
      "14403\n",
      "14404\n",
      "14405\n",
      "14406\n",
      "14407\n",
      "14408\n",
      "14409\n",
      "14410\n",
      "14411\n",
      "14412\n",
      "14413\n",
      "14414\n",
      "14415\n",
      "14416\n",
      "14417\n",
      "14418\n",
      "14419\n",
      "14420\n",
      "14421\n",
      "14422\n",
      "14423\n",
      "14424\n",
      "14425\n",
      "14426\n",
      "14427\n",
      "14428\n",
      "14429\n",
      "14430\n",
      "14431\n",
      "14432\n",
      "14433\n",
      "14434\n",
      "14435\n",
      "14436\n",
      "14437\n",
      "14438\n",
      "14439\n",
      "14440\n",
      "14441\n",
      "14442\n",
      "14443\n",
      "14444\n",
      "14445\n",
      "14446\n",
      "14447\n",
      "14448\n",
      "14449\n",
      "14450\n",
      "14451\n",
      "14452\n",
      "14453\n",
      "14454\n",
      "14455\n",
      "14456\n",
      "14457\n",
      "14458\n",
      "14459\n",
      "14460\n",
      "14461\n",
      "14462\n",
      "14463\n",
      "14464\n",
      "14465\n",
      "14466\n",
      "14467\n",
      "14468\n",
      "14469\n",
      "14470\n",
      "14471\n",
      "14472\n",
      "14473\n",
      "14474\n",
      "14475\n",
      "14476\n",
      "14477\n",
      "14478\n",
      "14479\n",
      "14480\n",
      "14481\n",
      "14482\n",
      "14483\n",
      "14484\n",
      "14485\n",
      "14486\n",
      "14487\n",
      "14488\n",
      "14489\n",
      "14490\n",
      "14491\n",
      "14492\n",
      "14493\n",
      "14494\n",
      "14495\n",
      "14496\n",
      "14497\n",
      "14498\n",
      "14499\n",
      "14500\n",
      "14501\n",
      "14502\n",
      "14503\n",
      "14504\n",
      "14505\n",
      "14506\n",
      "14507\n",
      "14508\n",
      "14509\n",
      "14510\n",
      "14511\n",
      "14512\n",
      "14513\n",
      "14514\n",
      "14515\n",
      "14516\n",
      "14517\n",
      "14518\n",
      "14519\n",
      "14520\n",
      "14521\n",
      "14522\n",
      "14523\n",
      "14524\n",
      "14525\n",
      "14526\n",
      "14527\n",
      "14528\n",
      "14529\n",
      "14530\n",
      "14531\n",
      "14532\n",
      "14533\n",
      "14534\n",
      "14535\n",
      "14536\n",
      "14537\n",
      "14538\n",
      "14539\n",
      "14540\n",
      "14541\n",
      "14542\n",
      "14543\n",
      "14544\n",
      "14545\n",
      "14546\n",
      "14547\n",
      "14548\n",
      "14549\n",
      "14550\n",
      "14551\n",
      "14552\n",
      "14553\n",
      "14554\n",
      "14555\n",
      "14556\n",
      "14557\n",
      "14558\n",
      "14559\n",
      "14560\n",
      "14561\n",
      "14562\n",
      "14563\n",
      "14564\n",
      "14565\n",
      "14566\n",
      "14567\n",
      "14568\n",
      "14569\n",
      "14570\n",
      "14571\n",
      "14572\n",
      "14573\n",
      "14574\n",
      "14575\n",
      "14576\n",
      "14577\n",
      "14578\n",
      "14579\n",
      "14580\n",
      "14581\n",
      "14582\n",
      "14583\n",
      "14584\n",
      "14585\n",
      "14586\n",
      "14587\n",
      "14588\n",
      "14589\n",
      "14590\n",
      "14591\n",
      "14592\n",
      "14593\n",
      "14594\n",
      "14595\n",
      "14596\n",
      "14597\n",
      "14598\n",
      "14599\n",
      "14600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14601\n",
      "14602\n",
      "14603\n",
      "14604\n",
      "14605\n",
      "14606\n",
      "14607\n",
      "14608\n",
      "14609\n",
      "14610\n",
      "14611\n",
      "14612\n",
      "14613\n",
      "14614\n",
      "14615\n",
      "14616\n",
      "14617\n",
      "14618\n",
      "14619\n",
      "14620\n",
      "14621\n",
      "14622\n",
      "14623\n",
      "14624\n",
      "14625\n",
      "14626\n",
      "14627\n",
      "14628\n",
      "14629\n",
      "14630\n",
      "14631\n",
      "14632\n",
      "14633\n",
      "14634\n",
      "14635\n",
      "14636\n",
      "14637\n",
      "14638\n",
      "14639\n",
      "14640\n",
      "14641\n",
      "14642\n",
      "14643\n",
      "14644\n",
      "14645\n",
      "14646\n",
      "14647\n",
      "14648\n",
      "14649\n",
      "14650\n",
      "14651\n",
      "14652\n",
      "14653\n",
      "14654\n",
      "14655\n",
      "14656\n",
      "14657\n",
      "14658\n",
      "14659\n",
      "14660\n",
      "14661\n",
      "14662\n",
      "14663\n",
      "14664\n",
      "14665\n",
      "14666\n",
      "14667\n",
      "14668\n",
      "14669\n",
      "14670\n",
      "14671\n",
      "14672\n",
      "14673\n",
      "14674\n",
      "14675\n",
      "14676\n",
      "14677\n",
      "14678\n",
      "14679\n",
      "14680\n",
      "14681\n",
      "14682\n",
      "14683\n",
      "14684\n",
      "14685\n",
      "14686\n",
      "14687\n",
      "14688\n",
      "14689\n",
      "14690\n",
      "14691\n",
      "14692\n",
      "14693\n",
      "14694\n",
      "14695\n",
      "14696\n",
      "14697\n",
      "14698\n",
      "14699\n",
      "14700\n",
      "14701\n",
      "14702\n",
      "14703\n",
      "14704\n",
      "14705\n",
      "14706\n",
      "14707\n"
     ]
    },
    {
     "ename": "TranslaterError",
     "evalue": "Failed to translate text! Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTranslaterError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-be38bcfad2ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meng\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mrus_transl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\yandex\\Translater.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTranslaterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Failed to translate text! {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTranslaterError\u001b[0m: Failed to translate text! Forbidden"
     ]
    }
   ],
   "source": [
    "# Using Yandex translater to create our target corpus\n",
    "# Determining how many we need\n",
    "rus = load_data('small_vocab_ru.txt')\n",
    "rus = rus.split('\\n')\n",
    "start_word = len(rus)\n",
    "print('Starting at position', start_word)\n",
    "\n",
    "# Doing the translation\n",
    "from yandex.Translater import Translater\n",
    "tr = Translater()\n",
    "tr.set_key('trnsl.1.1.20190304T033721Z.f33e8a2dae4367c5.c5ed7d57882a14129262a4ab776d539f6d934884')\n",
    "tr.set_from_lang('en')\n",
    "tr.set_to_lang('ru')\n",
    "rus_transl = rus\n",
    "for i in range(start_word, len(eng)):\n",
    "    print(i+1)\n",
    "    tr.set_text(eng[i])\n",
    "    rus_transl.append(tr.translate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our translations to a txt file\n",
    "f = open('small_vocab_ru.txt','w', encoding = 'utf-8')\n",
    "f.write('\\n'.join(rus_transl))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compare lines and make sure everything is correct\n",
    "eng_test = load_data('small_vocab_en.txt')\n",
    "rus_test = load_data('small_vocab_ru.txt')\n",
    "eng_test = eng_test.split('\\n')\n",
    "rus_test = rus_test.split('\\n')\n",
    "max_len = min([len(eng_test),len(rus_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france is never mild during spring , and it is sometimes pleasant in november .\n",
      "Франция никогда не бывает мягким в течение весны , и это иногда приятно в ноябре .\n"
     ]
    }
   ],
   "source": [
    "random_line = random.sample(range(0,max_len),1)[0]\n",
    "print(eng_test[random_line])\n",
    "print(rus_test[random_line])\n",
    "# Everything looks fine and lined up, we probably just need more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our reserved codes\n",
    "reserved_codes = {'<PAD>':0,\n",
    "                 '<EOS>':1,\n",
    "                 '<UNK>':2,\n",
    "                 '<GO>':3}\n",
    "\n",
    "# Creating our lookup table\n",
    "def setup_lookup_table(text, max_vocab=None):\n",
    "    # Starting the lookup table with the reserved codes\n",
    "    word_to_int = copy.copy(reserved_codes)\n",
    "    \n",
    "    # Determining the x most common words and replacing the others with '<UNK>'\n",
    "    if max_vocab is not None:\n",
    "        most_common = Counter(text.split(' ')).most_common(max_vocab)\n",
    "        text = ' '.join([x if x in most_common else '<UNK>' for x in text.split(' ')])\n",
    "    \n",
    "    # Tokenizing our sentences\n",
    "    vocabulary = set(text.split(' '))\n",
    "    \n",
    "    # Adding the rest of our vocab to the table\n",
    "    for i, word in enumerate(vocabulary, len(reserved_codes)):\n",
    "        word_to_int[word] = i\n",
    "        \n",
    "    # Creating the opposite table that converts ints to words\n",
    "    int_to_word = dict(zip(word_to_int.values(),word_to_int.keys()))\n",
    "    \n",
    "    return word_to_int, int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to tokenize our texts using the lookup tables\n",
    "def tokenize_text(eng_text, rus_text, eng_word_to_int, rus_word_to_int):\n",
    "    # First splitting texts into lists of sentences\n",
    "    eng_text = eng_text.split('\\n')\n",
    "    rus_text = rus_text.split('\\n')\n",
    "    \n",
    "    # Next we need to determine the longest sentences for padding\n",
    "    max_eng_length = max([len(x.split(' ')) for x in eng_text])\n",
    "    max_rus_length = max([len(x.split(' ')) for x in rus_text])\n",
    "    \n",
    "    # Empty lists for storing tokenized sentences\n",
    "    eng_token_list = []\n",
    "    rus_token_list = []\n",
    "    \n",
    "    # Tokenizing all of our sentences\n",
    "    for i in range(len(eng_text)):\n",
    "        # Tokenizing the sentence into a list of words\n",
    "        eng_words = eng_text[i].split(' ')\n",
    "        rus_words = rus_text[i].split(' ')\n",
    "        \n",
    "        # Replacing with <UNK>\n",
    "        eng_words = [x if x in eng_word_to_int else '<UNK>' for x in eng_words]\n",
    "        rus_words = [x if x in rus_word_to_int else '<UNK>' for x in rus_words]\n",
    "        \n",
    "        # Creating lists for int tokens\n",
    "        eng_tokens = [eng_word_to_int[x] for x in eng_words]\n",
    "        rus_tokens = [rus_word_to_int[x] for x in rus_words]\n",
    "        \n",
    "        # Appending the EOS token to the Russian sentences\n",
    "        rus_tokens.append(rus_word_to_int['<EOS>'])\n",
    "        \n",
    "        # Appending to the final sets\n",
    "        eng_token_list.append(eng_tokens)\n",
    "        rus_token_list.append(rus_tokens)\n",
    "        \n",
    "    return eng_token_list, rus_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our preprocessing function\n",
    "def preprocess_texts(eng_path, rus_path, subsample=None, max_vocab = None):\n",
    "    # Loading in our data\n",
    "    eng = load_data(eng_path)\n",
    "    rus = load_data(rus_path)\n",
    "    \n",
    "    # Putting the text to lowercase\n",
    "    eng = eng.lower()\n",
    "    rus = rus.lower()\n",
    "    \n",
    "    # Creating lists of sentences\n",
    "    eng = eng.split('\\n')\n",
    "    rus = rus.split('\\n')\n",
    "    \n",
    "    # Making sure we only take relevant sentences\n",
    "    max_len = min([len(eng), len(rus)])\n",
    "    eng = eng[0:max_len]\n",
    "    rus = rus[0:max_len]\n",
    "    \n",
    "    # Temp code for testing\n",
    "    #eng = eng[0:subsample]\n",
    "    #rus = rus[0:subsample]\n",
    "    #subsample = None\n",
    "    \n",
    "    # Taking a subsample if needed\n",
    "    if subsample is not None:\n",
    "        subsample = random.sample(range(len(eng)), subsample)\n",
    "        eng = [eng[x] for x in subsample]\n",
    "        rus = [rus[x] for x in subsample]\n",
    "    \n",
    "    # Padding our punctuation with whitespaces\n",
    "    eng = whitespace_punct(eng)\n",
    "    rus = whitespace_punct(rus)\n",
    "    \n",
    "    # Putting the texts back into a single text string\n",
    "    eng = '\\n'.join(eng)\n",
    "    rus = '\\n'.join(rus)\n",
    "    \n",
    "    # Creating the lookup tables\n",
    "    eng_word_to_int, eng_int_to_word = setup_lookup_table(eng, max_vocab)\n",
    "    rus_word_to_int, rus_int_to_word = setup_lookup_table(rus, max_vocab)\n",
    "    \n",
    "    # Creating the tokenized texts\n",
    "    eng_tokenized, rus_tokenized = tokenize_text(eng, rus, eng_word_to_int, rus_word_to_int)\n",
    "\n",
    "    # Saving the data with pickle\n",
    "    pickle.dump((\n",
    "        (eng_tokenized, rus_tokenized),\n",
    "        (eng_word_to_int, rus_word_to_int),\n",
    "        (eng_int_to_word, rus_int_to_word)), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our preprocessed data\n",
    "#preprocess_texts('1mcorpus/english_corpus.txt', '1mcorpus/russian_corpus.txt', subsample=1000, max_vocab=None)\n",
    "preprocess_texts('small_vocab_en.txt', 'small_vocab_ru.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our preprocessed texts\n",
    "def load_preprocess():\n",
    "    with open('preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our preprocessed data\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m100\u001b[0m\n\u001b[1;33m    def __call__(self, inputs, state, scope=None):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Defining our layer normed GRU cell\n",
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"GRU cell implementation for the skip-thought vectors model.\"\"\"\n",
    "\n",
    "_layer_norm = tf.contrib.layers.layer_norm\n",
    "\n",
    "class LayerNormGRUCell(tf.contrib.rnn.RNNCell):\n",
    "    \"\"\"\n",
    "    GRU cell with layer normalization.\n",
    "    The layer normalization implementation is based on:\n",
    "    https://arxiv.org/abs/1607.06450.\n",
    "    \"Layer Normalization\"\n",
    "    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               num_units,\n",
    "               w_initializer,\n",
    "               u_initializer,\n",
    "               b_initializer,\n",
    "               activation=tf.nn.tanh):\n",
    "        \"\"\"Initializes the cell.\n",
    "\n",
    "        Args:\n",
    "          num_units: Number of cell units.\n",
    "          w_initializer: Initializer for the \"W\" (input) parameter matrices.\n",
    "          u_initializer: Initializer for the \"U\" (recurrent) parameter matrices.\n",
    "          b_initializer: Initializer for the \"b\" (bias) parameter vectors.\n",
    "          activation: Cell activation function.\n",
    "        \"\"\"\n",
    "        self._num_units = num_units\n",
    "        self._w_initializer = w_initializer\n",
    "        self._u_initializer = u_initializer\n",
    "        self._b_initializer = b_initializer\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def _w_h_initializer(self):\n",
    "        \"\"\"Returns an initializer for the \"W_h\" parameter matrix.\n",
    "\n",
    "        See equation (23) in the paper. The \"W_h\" parameter matrix is the\n",
    "        concatenation of two parameter submatrices. The matrix returned is\n",
    "        [U_z, U_r].\n",
    "\n",
    "        Returns:\n",
    "          A Tensor with shape [num_units, 2 * num_units] as described above.\n",
    "        \"\"\"\n",
    "\n",
    "        def _initializer(shape, dtype=tf.float32, partition_info=None):\n",
    "            num_units = self._num_units\n",
    "            assert shape == [num_units, 2 * num_units]\n",
    "            u_z = self._u_initializer([num_units, num_units], dtype, partition_info)\n",
    "            u_r = self._u_initializer([num_units, num_units], dtype, partition_info)\n",
    "            return tf.concat([u_z, u_r], 1)\n",
    "\n",
    "        return _initializer\n",
    "\n",
    "    def _w_x_initializer(self, input_dim):\n",
    "        \"\"\"Returns an initializer for the \"W_x\" parameter matrix.\n",
    "\n",
    "        See equation (23) in the paper. The \"W_x\" parameter matrix is the\n",
    "        concatenation of two parameter submatrices. The matrix returned is\n",
    "        [W_z, W_r].\n",
    "\n",
    "        Args:\n",
    "          input_dim: The dimension of the cell inputs.\n",
    "\n",
    "        Returns:\n",
    "          A Tensor with shape [input_dim, 2 * num_units] as described above.\n",
    "        \"\"\"\n",
    "\n",
    "        def _initializer(shape, dtype=tf.float32, partition_info=None):\n",
    "            num_units = self._num_units\n",
    "            assert shape == [input_dim, 2 * num_units]\n",
    "            w_z = self._w_initializer([input_dim, num_units], dtype, partition_info)\n",
    "            w_r = self._w_initializer([input_dim, num_units], dtype, partition_info)\n",
    "            return tf.concat([w_z, w_r], 1)\n",
    "\n",
    "        return _initializer\n",
    "\n",
    "      def __call__(self, inputs, state, scope=None):\n",
    "            \"\"\"GRU cell with layer normalization.\"\"\"\n",
    "            input_dim = inputs.get_shape().as_list()[1]\n",
    "            num_units = self._num_units\n",
    "\n",
    "    with tf.variable_scope(scope or \"gru_cell\"):\n",
    "      with tf.variable_scope(\"gates\"):\n",
    "        w_h = tf.get_variable(\n",
    "            \"w_h\", [num_units, 2 * num_units],\n",
    "            initializer=self._w_h_initializer())\n",
    "        w_x = tf.get_variable(\n",
    "            \"w_x\", [input_dim, 2 * num_units],\n",
    "            initializer=self._w_x_initializer(input_dim))\n",
    "        z_and_r = (_layer_norm(tf.matmul(state, w_h), scope=\"layer_norm/w_h\") +\n",
    "                   _layer_norm(tf.matmul(inputs, w_x), scope=\"layer_norm/w_x\"))\n",
    "        z, r = tf.split(tf.sigmoid(z_and_r), 2, 1)\n",
    "      with tf.variable_scope(\"candidate\"):\n",
    "        w = tf.get_variable(\n",
    "            \"w\", [input_dim, num_units], initializer=self._w_initializer)\n",
    "        u = tf.get_variable(\n",
    "            \"u\", [num_units, num_units], initializer=self._u_initializer)\n",
    "        h_hat = (r * _layer_norm(tf.matmul(state, u), scope=\"layer_norm/u\") +\n",
    "                 _layer_norm(tf.matmul(inputs, w), scope=\"layer_norm/w\"))\n",
    "      new_h = (1 - z) * state + z * self._activation(h_hat)\n",
    "    return new_h, new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing model inputs\n",
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our hyper parameter input function\n",
    "def hyperparam_inputs():\n",
    "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return lr_rate, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function for processing input to the decoder, intializing with '<GO>'\n",
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the encoding layer\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    cell_list = [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LayerNormBasicLSTMCell(rnn_size), keep_prob) for _ in range(num_layers)]\n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training process for decoding layer\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference process for the decoding layer\n",
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "    # Replicate encoder infos beam_width times\n",
    "    #decoder_initial_state = tf.contrib.seq2seq.tile_batch(\n",
    "    #    encoder_state, multiplier=beam_width)\n",
    "\n",
    "    # Define a beam-search decoder\n",
    "    #decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "    #        cell=dec_cell,\n",
    "    #        embedding=dec_embeddings,\n",
    "    #        start_tokens=start_of_sequence_id,\n",
    "    #        end_token=end_of_sequence_id,\n",
    "    #        initial_state=decoder_initial_state,\n",
    "    #        beam_width=beam_width,\n",
    "    #        output_layer=output_layer,\n",
    "    #        length_penalty_weight=0.0,\n",
    "    #        coverage_penalty_weight=0.0)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the decoding layer\n",
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cell_list = [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LayerNormBasicLSTMCell(rnn_size), keep_prob) for _ in range(num_layers)]\n",
    "    cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and build the seq2seq model\n",
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence model\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "display_step = 100\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the graph\n",
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches and padding\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  100/229 - Train Accuracy: 0.2673, Validation Accuracy: 0.2844, Loss: 3.1230\n",
      "Epoch   0 Batch  200/229 - Train Accuracy: 0.4062, Validation Accuracy: 0.4086, Loss: 2.4104\n",
      "Epoch   1 Batch  100/229 - Train Accuracy: 0.4038, Validation Accuracy: 0.4625, Loss: 2.0323\n",
      "Epoch   1 Batch  200/229 - Train Accuracy: 0.3539, Validation Accuracy: 0.3727, Loss: 1.5565\n",
      "Epoch   2 Batch  100/229 - Train Accuracy: 0.3125, Validation Accuracy: 0.3250, Loss: 1.4665\n",
      "Epoch   2 Batch  200/229 - Train Accuracy: 0.3312, Validation Accuracy: 0.2992, Loss: 1.1472\n",
      "Epoch   3 Batch  100/229 - Train Accuracy: 0.3816, Validation Accuracy: 0.3883, Loss: 1.2040\n",
      "Epoch   3 Batch  200/229 - Train Accuracy: 0.4289, Validation Accuracy: 0.4383, Loss: 0.9691\n",
      "Epoch   4 Batch  100/229 - Train Accuracy: 0.4433, Validation Accuracy: 0.4414, Loss: 1.0334\n",
      "Epoch   4 Batch  200/229 - Train Accuracy: 0.5250, Validation Accuracy: 0.4695, Loss: 0.8656\n",
      "Epoch   5 Batch  100/229 - Train Accuracy: 0.4696, Validation Accuracy: 0.4945, Loss: 1.0071\n",
      "Epoch   5 Batch  200/229 - Train Accuracy: 0.5391, Validation Accuracy: 0.4844, Loss: 0.8118\n",
      "Epoch   6 Batch  100/229 - Train Accuracy: 0.4942, Validation Accuracy: 0.4680, Loss: 0.9261\n",
      "Epoch   6 Batch  200/229 - Train Accuracy: 0.5523, Validation Accuracy: 0.5125, Loss: 0.7556\n",
      "Epoch   7 Batch  100/229 - Train Accuracy: 0.5123, Validation Accuracy: 0.5062, Loss: 0.8766\n",
      "Epoch   7 Batch  200/229 - Train Accuracy: 0.5930, Validation Accuracy: 0.5109, Loss: 0.7136\n",
      "Epoch   8 Batch  100/229 - Train Accuracy: 0.5156, Validation Accuracy: 0.5141, Loss: 0.8479\n",
      "Epoch   8 Batch  200/229 - Train Accuracy: 0.5914, Validation Accuracy: 0.5367, Loss: 0.6821\n",
      "Epoch   9 Batch  100/229 - Train Accuracy: 0.5444, Validation Accuracy: 0.5492, Loss: 0.7928\n",
      "Epoch   9 Batch  200/229 - Train Accuracy: 0.5953, Validation Accuracy: 0.5578, Loss: 0.6706\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# Training the actual model\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading model parameters\n",
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    with open('params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading from our checkpoint\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "\n",
      "English Input: my least liked fruit is the lemon , but her least liked is the peach .\n",
      "Russian Prediction: наименее понравился фрукт - это банан , но его не понравилось - это лимон . <EOS>\n",
      "Actual Russian:  мне меньше понравился фруктов является лимон , но ей не понравилось это персик .\n"
     ]
    }
   ],
   "source": [
    "# Applying the translation\n",
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "random_line = random.sample(range(0,max_len),1)[0]\n",
    "\n",
    "translate_sentence = eng_test[random_line]\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('\\nEnglish Input: {}'.format(\" \".join([source_int_to_vocab[i] for i in translate_sentence])))\n",
    "print('Russian Prediction: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n",
    "print('Actual Russian: ', rus_test[random_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
